<!-- ---
layout: post
title:  Resolving Objections to Utilitarianism
date:   2020-04-20 12:05:20 +0300
image:  /assets/images/blog/post-7.jpg
author: Jack Ryan
tags:   ethics
---

**I wrote this up pretty quickly and would love feedback; when I wrote this, my imagined audience was my facebook friends, so fair-warning that this isn't a very advanced topic. Also, after writing this, I had some ideas about how to resolve the repugnant conclusion argument against utilitarianism but I didn't write it here (tl;dr is that we can't imagine what it would be like to be so certain about what is valuable that we would choose the repugnant conclusion world over the current world. If we actually could be certain about the utility of a person's life to many decimals places and for such small values, then the repugnant conclusion wouldn't be so repugnant. I just really can't imagine us having such high precision/certainty over that sort of thing). Okay here are some thoughts on stuff:**

A lot of people seem to not like utilitarianism because they can think of versions of it that are bad. But to me, this is like saying "I don't like sports because golf is boring." The framework of utilitarianism is still a good one! If you have preferences over how you would like the world to be (for any two worlds, either one is better than the other or you are indifferent between the worlds, and A > B,  B > C implies A > C), then there is a utility function which fully represents your values. Out of the actions that you can think of you then choose the action that will maximize the expected utility over all of space and time.

Let's say world A is our universe, and world B is our universe except the population is 1 and this person is 10^30 times happier than the average person. If you think utilitarianism is bad because it concludes that world B has more utility than world A, take note that this is not the fault of utilitarianism, but incorrect evaluation of the utility function or even bad choice of utility function.

Under a reasonable utility function, world B has less utility over all of time and space than world A. Why? Because a single person has much less instrumental utility than humanity as a whole; in the universe with only one person, humanity has no future and that person's actions have almost no importance for the future. In our world, humanity has incredible potential that it probably will achieve which outweighs the 10^30 happiness by most reasonable measures of happiness I can think of (see Bostrom's estimates of future lives), and this potential is more reliably achieved with a larger and less vulnerable society. Even a world with 100 really happy people is undesirable under a reasonable utility function, because of the risks that society would face.

On the other hand, you could compare world A with world B' where the really happy person is also immortal. If you feel like utilitarianism would force you to choose world B and you don't agree with this, then your true rejection might be something like "I don't think scaling a single person's happiness by a certain factor is better than scaling the number of happy people" which is still consistent with utilitarianism! Maybe your utility function only increases by a factor of 2 when a person's happiness increases by a factor of 10. Or you might say "I don't know enough about what I value and what it would actually mean to be '10^30 times happier' to reliably say which of these worlds is preferable" which is also okay; you should just hope you are never actually faced with this decision, otherwise you'd be forced to choose one of these worlds.

You might also just choose the world we live in because making big changes is risky and comes with uncertainty, and utilitarianism also takes this into account! That intuition would be encapsulated in the "expected" part of the "expected utility" calculation. When you are uncertain about the value of a possible world, the utility gets down-weighted with your uncertainty.

I still haven't heard objections to utilitarianism that aren't a result of improperly evaluating the expected utility of a choice over all of time and space or just bad choice of utility function. I guess if you intrinsically value following certain moral rules or having certain virtues instead of valuing certain physical states of the universe (like one where there's no conflict, disease, poverty, violence, etc and lots of human welfare), then maybe utilitarianism isn't so useful to you. I guess that would correspond to your utility function being maximized if and only if you follow all the rules and/or are virtuous. -->