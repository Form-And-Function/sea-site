---
layout: post
title:  For Total Hedonism
date:   2019-03-9 12:05:55 +0300
image:  /assets/images/blog/post-8.jpg
author: Mauricio Baker
tags:   ethics
---

**Might be preaching to the choir, or things that have been said before. Anyway, I talk about four things here; feel free to skip to whichever part interests you. In the first section, I argue that identifying extreme happiness with moments that seem infinite is legit and makes hedonism much more appealing. Then, I discuss how I find it odd to care about anything other than subjective well-being. Lastly, I argue that two supposedly repugnant conclusions of aggregative hedonism are actually not bad. These are basically the reasons and intuitions that make me think that classical aggregative hedonistic utilitarianism, rather than other forms of utilitarianism, best reflects what I most care about upon reflection.**

Happiness is Underrated (Seriously)

"Then she soars further to a blissful place of which I neither will nor can speak. It is too difficult; I do not dare." - Mechthild, The Flowing Light of Divinity

"And there in life... there if anywhere should a person live his life, beholding that Beauty. If you once see that, it wonâ€™t occur to you to measure beauty by [any lesser standard]." - Plato, Symposium

Sometimes people say stuff like that. Sometimes, people say they experienced such a moment, and then spent much of the rest of their lives trying to return to it. Why? Part of me wants to say that they're making stuff up to confuse critics and attract followers, but I think that would be overly dismissive. Because something resonates in that kind of talk. There have a few moments in my life, a few moments of feeling so fully alive, that they seem to have been experiences beyond ordinary sensation, moments impossible to describe as we normally describe things. Moments of such inspiration that they make me want to say that, for an instant, I touched the infinite. 

I'm normally pretty skeptical of that kind of talk. So what feelings are people talking about, when they talk about "touching the eternal"? The best naturalistic explanation I can think of (at least for a lot of these moments) is that this is what intense bliss feels like. A lot of people think that what we call "intense suffering" is orders of magnitude worse than what we call "mild suffering," and I'm inclined to agree. It's not hard for us to evoke horror in ourselves at intense suffering--we can imagine horrible torture (or find videos of factory farms). The kind of experiences that people talk about as "experiencing the infinite" seem to be the opposite of this: happiness orders of magnitude better than ordinary happiness. The right tail of the curve of possible human happiness.

Thinking about extreme happiness as seemingly infinite moments, rather than as orgasms or whatever, changes my intuitions about a bunch of ethical dilemmas / thought experiments:

It makes me somewhat sympathetic to the idea that experiencing such a moment of absolute bliss would be so good that it would be worth bearing any suffering. I feel similarly sympathetic to the idea that some suffering is so bad that no amount of happiness can make up for it. Since these two compelling intuitions are totally incompatible, I adopt a weaker version of both: the extremes of suffering and happiness are both super important, but neither is so important that nothing can make up for it. The symmetrical appeal of reducing extreme suffering and increasing extreme bliss (when extreme bliss is conceived of in its most compelling form) makes me value the two similarly.

- It makes me see the idea of filling most* of the reachable universe with super happy computers, not as a repugnant conclusion, but as an exciting one. Doing less seems like such a loss of potential.

- It makes me feel much better about feeding utility monsters. If we come across any that is looking for a meal, sign me up!

- It makes me rethink the experience machine. People sometimes think that hedonists would love to have everyone plugged into an upgraded Matrix, living fake lives of joy. No--any narratives about one's life, true or false, would use up valuable resources that could be converted into unspeakable bliss. Again, sign me up!

* (I support "most" rather than "all" for cooperative purposes. Non-aggregationists could have some solar systems with beauty / truth / complexity / more conventional standards of human flourishing--it means a lot to them, and it's a relatively small cost for hedonists. A nice thing about cooperating with non-aggregationists is that they're often easier to please.)

In short, lack of enthusiasm for hedonism seems like it's partially the result of imagining extreme happiness in ways that sell it short. The "repugnant conclusions" of hedonism start to sound much more appealing when I identify extreme bliss, not with orgasms or whatever, but with seemingly infinite moments: with the most profoundly inspiring, fulfilling aspects of human experience. Conventional standards of flourishing pale in comparison.


Some other thoughts on ethics:

Happiness as Subjective Well-Being as The Only Thing That Matters

(When I say "happiness," I just mean: whatever is subjectively experienced as good.)

Eliezer writes that he does not only care about happiness; he cares about many things other than how something is subjectively experienced. I find this position bizarre. The idea that a world could be better than another world (by my lights), even if no being subjectively experienced it as better, seems really odd to me--I don't get where people who feel that way are coming from. Better for whom? For me, judging it from the outside? I couldn't endorse my own judgement in that case--it would seem too detached from the subjective experiences I care about. 

This inclines me towards hedonism rather than other value systems--insofar as preference satisfaction, freedom, beauty, love, etc aren't subjectively experienced as good (and don't bring about experiences that are subjectively felt to be good), I don't find caring about them compelling.

Some may suggest contrary intuitions by asking, "would you still value a being's happiness if the being didn't prefer to be happier?" I don't think that question makes sense, given the above definition of happiness. Any reasonable account of preference will not leave room for someone to "prefer" to subjectively experience the world as worse, as an end in itself. Who in their right mind would prefer that? What reasonable account of preference would value such a nonsensical preference? So the question is almost like asking, "would you still use standard math if 1+1=3?" It's a necessarily impossible counterexample, given what we mean by our terms.


Not-So-Repugnant Conclusion #1:

One argument that people sometimes make against utilitarianism is the following:

(Aggregate) utilitarianism implies that, given any population of people living amazing lives, it would be better for there to instead be a sufficiently larger population of people who have lives barely worth living, because their aggregate happiness would be greater if there were enough of them.

Intuitively, that seems terrible. But I think this intuitive recoil comes mostly from misleading wording.

While I've been lucky enough to have never seriously considered suicide, I would guess that people decide whether to continue living by thinking both about their own future well-being and the effects that their decision to live has on others' well-being. A life that, overall, wouldn't quite be worth it for oneself may be well worth it for oneself + the people one cares about. Since people often care quite a lot for those they love, who would be very sad if one died, the extent to which a given life is worth choosing to live is probably significantly greater than the extent to which it is worth experiencing. 

Most of why the repugnant conclusion argument is misleading comes from how it mixes these up. When we hear "lives barely worth living," we might imagine "lives barely worth choosing to live." Our image of "lives barely worth choosing to live" is rightfully gloomy. However, because positive externalities usually support the choice to live, "a life barely worth choosing to live" would probably not be a life worth experiencing. In other words, it would probably involve net negative well-being. So such a world would not be logically implied by utilitarianism--it would be the aggregate of many net negative lives, and therefore far worse, from the perspective of a utilitarian, than a world of a few happy people. 

As a result, the repugnant conclusion argument doesn't work if we interpret its wording as it's easy to interpret it. It only works, logically, if "lives barely worth living" is taken to mean lives that are just slightly worth experiencing, and therefore well worth choosing to live for oneself + others. I'd be surprised if this didn't accurately describe many people today, whose existence we think of as fine, rather than as a repugnant conclusion.

(Making our initial intuitions about this repugnant conclusion even more misleading, we're probably evolutionarily inclined to perceive lives as barely worth living only once they're well below net zero well-being. So lives that we imagine as barely worth living are probably much worse than lives that are actually worth living, from a utilitarian standpoint.)


Not-So-Repugnant Conclusion #2:

On the other side of the suffering-enjoyment scale, the repugnant conclusion that people sometimes use to argue against utilitarianism is the following:

Dilemma 1:

If one had to choose between:

     A) one person is horribly tortured for 50 years,

     B) X people get barely painful dust specks in their eyes

(Aggregate) utilitarianism implies that there is a sufficiently large X such that one should choose A, because the aggregate suffering of enough dust specks would be greater.

Something that changed my initial intuitive recoil was thinking along these lines:

Let's forget our existing hypothetical for a minute, and use a new one: 

Dilemma 2: I have to choose between the following:

     A) one person is mildly tortured for 1 second, with a probability of 0.0001%

     B) X people get 1,000 dust specks in their eyes, every day for the rest of their lives, with near certainty.

I would easily say that, in this case, a large enough number of dust speck victims (maybe just 1?) would make option A better.

Now let's gradually change Dilemma 2. Let's gradually decrease, one by one, the number of dust specks each dust speck victim gets from 1,000 to 1, then the number of days when this happens to just 1. Then, looking at option A, let's gradually increase the hypothetical torture length from 1 second to 50 years, then the intensity from mild to horrible, and the probability from 0.0001% to near certainty. We've now gotten back to the original dilemma. This puts people's original intuitions in a tough spot. Because if enough dust specks would outweigh torture in Dilemma 2, and no number of dust specks would outweigh torture in the original dilemma, then there must have been some point in our gradual transition from the new dilemma back to the old dilemma where this changed. That is, there must have been two dilemmas that were just minutely different, and torture was potentially outweighable by dust specks in one of the situations but not in the other. That's pretty weird.

And no, one can't say that change happened gradually. The question "would enough dust speck victims be worse than the torture?" has two discrete answers: "yes," and "no." The dilemma can change gradually, but answers to that question don't have room to change gradually.

So we have three intuitions that can't all work together:

In the original dust speck dilemma, no number of dust speck victims would be worse than the torture.

In Dilemma 2, enough dust speck victims would be worse than the torture.

Tiny differences between dilemmas do not determine whether enough dust specks would be worse than torture.

You can't have all three because, as argued above, 1 and 2 together imply that there is a sharp threshold around which tiny differences determine whether enough dust specks outweigh torture.

One intuition has to get thrown out. Which will it be? I vote for throwing out the intuition that should already have been highly suspect.

So we accept this conclusion as, not really repugnant, but seemingly repugnant, because we can't imagine anything close to the number of dust specks that would take.